{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori算法进行关联分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本概念\n",
    "### 关联分析\n",
    "从大规模数据集中寻找物品间的隐含关系，或称关联规则学习（association rule learning）\n",
    "### 频繁项集（frequent item sets）\n",
    "经常出现在一块的物品的集合\n",
    "### 关联规则\n",
    "暗示两种物品间可能存在很强的关系\n",
    "### 支持度（support）\n",
    "一个项集的支持度被定义为数据集中包含该项集的记录所占的比例\n",
    "support({A,B}) = num(A∪B) / W = P(A∩B)\n",
    "num(A∪B)表示含有物品集{A,B}的事务集的个数，不是数学中的并集。\n",
    "支持度是针对项集来说的，因此可以定义一个最小支持度，而只保留满足最小支持度的项集\n",
    "### 置信度（confidence）\n",
    "置信度(confidence)揭示了A出现时B是否一定出现，如果出现，则出现的概率是多大。\n",
    "Confidence(A->B) = support({A,B}) / support({A}) = P(B|A)\n",
    "支持度和置信度是用来量化关联分析是否成功的方法\n",
    "\n",
    "### Apriori原理\n",
    "如果某个项集是频繁的，那么它的所有子集也是频繁的，其逆否命题：\n",
    "如果一个项集是非频繁集，则它的所有超集都是非频繁的\n",
    "作用：减少关联规则学习时所需的计算量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "扫描数据集生成候选项集伪代码如下\n",
    "对数据集中的每条交易记录tran：\n",
    "    检查一下can是否tran的子集：\n",
    "    是则增加can的计数值\n",
    "对每个候选项集：\n",
    "    如果其支持度不低于最小值，则保留该项集\n",
    "    返回所有频繁项列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "def loadDataSet():                                                                         # 用于测试的简单数据集\n",
    "    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createC1(dataSet):                                                                     # 构建大小为1的所有候选集的集合\n",
    "    C1 = []\n",
    "    for transaction in dataSet:                                                            # 遍历数据集中的每条记录\n",
    "        for item in transaction:                                                           # 遍历每条记录中的每个物品\n",
    "            if not [item] in C1:                                                           \n",
    "                C1.append([item])                                                          # C1中还不存在的情况下，加入C1中\n",
    "    C1.sort()                                                                              # 排序\n",
    "    return list(map(frozenset, C1)) # use frozen set so we can use it as a key in a dict   # 返回 对C1中每个项构建一个不变集合    只包含一个物品项的列表，python不能创建只有一个整数的集合 \n",
    "                                    # frozenset类型与set类型均不能被用户所修改，但前者可以将这些集合作为字典的key，后者不能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scanD(D, Ck, minSupport):                                                           # 该函数用于从候选项集Ck生成满足最小支持度的频繁项Lk\n",
    "    '''input:  D                数据集的每一条记录的集合\n",
    "               Ck               候选项集列表Ck\n",
    "               minSupport       最小支持度\n",
    "        output：retList          最频繁项集Lk（dict，key为项集组合，value为计数）\n",
    "                supportData      Lk的项集的支持度（dict，key为项集组合，value为其对应的支持度）\n",
    "    '''                                                                                 # 计数\n",
    "    ssCnt = {}                                                                          # 创建空字典\n",
    "    for tid in D:                                                                       # 遍历每一条交易记录\n",
    "        for can in Ck:                                                                  # 遍历候选集Ck\n",
    "            if can.issubset(tid):                                                       # 如果Ck中的集合是记录的一部分\n",
    "                                                                                        # 增加字典中候选集key对应的计数值\n",
    "                if can not in ssCnt: ssCnt[can]=1                                       # 情况1： 第一次记录\n",
    "                else: ssCnt[can] += 1                                                   # 情况2：不是第一次\n",
    "                                                                                        # 遍历完成得到字典ssCnt，key为Ck项集组合，value为计数值\n",
    "                                                                                        # 当扫描完数据集中所有项以及所有候选集时，得到key为候选项，value为其计数的字典ssCnt\n",
    "                                                                                       \n",
    "                                                                                        # 计算支持度，筛掉不满足支持度的项\n",
    "    numItems = float(len(D))                                                            # 总交易记录数         \n",
    "    retList = []                                                                        # 初始化Lk\n",
    "    supportData = {}                                                                    # 初始化Lk中每一个项的支持度\n",
    "    for key in ssCnt:                                                                   # 遍历字典\n",
    "        support = ssCnt[key]/numItems                                                   # 计算支持度\n",
    "        if support >= minSupport:                                                       # 若支持度大于设定的最小支持度\n",
    "            retList.insert(0,key)                                                       # 加入relist字典\n",
    "        supportData[key] = support                                                      \n",
    "    return retList, supportData                                                         # 返回最频繁项集Lk和支持度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({1}),\n",
       " frozenset({2}),\n",
       " frozenset({3}),\n",
       " frozenset({4}),\n",
       " frozenset({5})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#尝试\n",
    "dataSet = loadDataSet()\n",
    "C1 = createC1(dataSet)\n",
    "C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1, 3, 4}, {2, 3, 5}, {1, 2, 3, 5}, {2, 5}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = list(map(set, dataSet)) # 将数据集的每一条交易记录变为集合\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1, suppData0 = scanD(D, C1, 0.5)\n",
    "L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{frozenset({1}): 0.5,\n",
       " frozenset({3}): 0.75,\n",
       " frozenset({4}): 0.25,\n",
       " frozenset({2}): 0.75,\n",
       " frozenset({5}): 0.75}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppData0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组织完整的Apriori算法\n",
    "伪代码如下\n",
    "当集合中项的个数大于0时：\n",
    "    构建一个k个项组成的候选项集的列表\n",
    "    检查数据确认每个项集都是频繁的\n",
    "    保留频繁项集并构建k+1项组成的候选项集的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aprioriGen(Lk, k):                                                         # 由最频繁集Lk创建其超集Ck+1\n",
    "    '''input:  Lk              频繁项集列表\n",
    "               k               生成的项集的项的元素个数\n",
    "       output：retList         创建下一轮的候选集Ck+1\n",
    "    '''\n",
    "    retList = []                                                              # 初始化超集列表\n",
    "    lenLk = len(Lk)                                                           # 计算Lk中项的个数\n",
    "    for i in range(lenLk):                                                    # Lk中的频繁项集进行两两比较\n",
    "        for j in range(i+1, lenLk): \n",
    "            L1 = list(Lk[i])[:k-2]                                            # 比较两个项的前k-2个元素，若相等即可合并成新的项，不满足则有两种情况1）已考虑过的超集  2）其子集非频繁（）Apriori原理的逆否命题\n",
    "            L2 = list(Lk[j])[:k-2]\n",
    "            L1.sort()\n",
    "            L2.sort()\n",
    "            if L1==L2:                                                        # if first k-2 elements are equal\n",
    "                retList.append(Lk[i] | Lk[j])                                 # set union  并加入超集列表                    \n",
    "    return retList                                                            # 返回超集列表                                            \n",
    "# 举例， 以{0}{1}{2}作为输入，会生成{0,1}{0,2}{1,2}\n",
    "# k-2的原因，{0,1}{0,2}{1,2}生成三元素集合{0,1,2}仅进行一次操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(dataSet, minSupport = 0.5):                                      # Apriori 算法\n",
    "    '''input:  dataSet               数据集\n",
    "               minSupport = 0.5      最小支持度\n",
    "       output：retList                Ck+1\n",
    "    '''\n",
    "    C1 = createC1(dataSet)                                                   # 数据集创建C1\n",
    "    D = list(map(set, dataSet))                                              # 数据集记录生成记录集合列表\n",
    "    L1, supportData = scanD(D, C1, minSupport)                               # scanD生成L1及对应的支持度\n",
    "    L = [L1]                                                                 # 初始化总的最频繁集L\n",
    "    k = 2\n",
    "    while (len(L[k-2]) > 0):                                                 # 有了L1，继续寻找L2,3....直至下一个大的项集为空\n",
    "        Ck = aprioriGen(L[k-2], k)                                           # aprioriGen 由Lk生成候选集Ck+1\n",
    "        Lk, supK = scanD(D, Ck, minSupport)                                  # scan DB to get Lk，丢掉不满足最小支持度要求的项，  由Ck+1生成Lk+1   \n",
    "        supportData.update(supK)                                             # 更新支持度 （ update() 函数把字典dict2的键/值对更新到dict里 ）                   \n",
    "        L.append(Lk)                                                         # 总的最频繁项集L中加入Lk+1\n",
    "        k += 1                                                               \n",
    "    return L, supportData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})],\n",
       " [frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5}), frozenset({1, 3})],\n",
       " [frozenset({2, 3, 5})],\n",
       " []]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, suppData = apriori(dataSet)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5}), frozenset({1, 3})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[frozenset({2, 3, 5})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个项集都是在apriori()中调用函数aprioriGen()来生成的\n",
    "# aprioriGen的工作流程\n",
    "aprioriGen(L[1],3)\n",
    "# 因为{1，2}{1，3}{1，5}均为非频繁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[frozenset({5}), frozenset({2}), frozenset({3})], [frozenset({2, 5})], []]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, suppData = apriori(dataSet, minSupport=0.7)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从频繁项集中挖掘关联规则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRules(L, supportData, minConf=0.7):                                          # supportData is a dict coming from scanD\n",
    "    '''input  L                   频繁项集列表L\n",
    "              supportData         包含那些频繁项集支持数据的字典 （每个项的支持度）\n",
    "              minConf=0.7         最小可行度阈值\n",
    "       output bigRuleList         包含可信度的规则列表\n",
    "    '''\n",
    "    bigRuleList = []                                                                     # 初始化关联规则列表\n",
    "    for i in range(1, len(L)):                                                           # only get the sets with two or more items\n",
    "        for freqSet in L[i]:                                                             # 遍历L[i]中的每一项\n",
    "            H1 = [frozenset([item]) for item in freqSet]                                 # 遍历L[i]每一项中的每一元素，（作为rulesFromConseq或rulesFromConseq的出现在规则右部的元素列表H）\n",
    "            if (i > 1):                                                                  # L[2]及其2以上的每一项中的元素个数大于2，需要先生成候选规则集\n",
    "                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)          # 调用rulesFromConseq，生成候选规则集合\n",
    "                \n",
    "            else:                                                                        # L[1]中每一项仅有两个元素，不需要生成规则，可以直接计算可信度并评价\n",
    "                calcConf(freqSet, H1, supportData, bigRuleList, minConf)                 # 调用calcConf，对规则进行评价\n",
    "    return bigRuleList         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcConf(freqSet, H, supportData, brl, minConf=0.7):                                 # 对生成的规则进行评估\n",
    "    '''input  freqSet           L[i]中的每一项\n",
    "              H                 出现在规则右部的元素列表H\n",
    "              supportData       含那些频繁项集支持数据的字典 （每个项的支持度）\n",
    "              brl               包含可信度的规则列表\n",
    "              minConf=0.7       最小可信度\n",
    "       output prunedH           满足最小可信度的规则列表\n",
    "    '''\n",
    "    prunedH = []                                                                       # create new list to return\n",
    "    for conseq in H:\n",
    "        conf = supportData[freqSet]/supportData[freqSet-conseq]                        # calc confidence\n",
    "        if conf >= minConf: \n",
    "            print(freqSet-conseq,'-->',conseq,'conf:',conf)\n",
    "            brl.append((freqSet-conseq, conseq, conf))\n",
    "            prunedH.append(conseq)                                                    # 满足最小可信度的规则保存在 prunedH中\n",
    "    return prunedH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):                        # 生成候选规则集合\n",
    "    '''input  freqSet           L[i]中的每一项\n",
    "              H                 出现在规则右部的元素列表H\n",
    "              supportData       含那些频繁项集支持数据的字典 （每个项的支持度）\n",
    "              brl               包含可信度的规则列表\n",
    "              minConf=0.7       最小可信度\n",
    "       output rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)               递归\n",
    "    '''\n",
    "    m = len(H[0])\n",
    "    if (len(freqSet) > (m + 1)):                                                     # try further merging  如果L[i]中的每一项的元素个数大于2，则出现在规则右部的元素列表H需要进一步考虑合并元素\n",
    "        Hmp1 = aprioriGen(H, m+1)                                                    # create Hm+1 new candidates 对H寻找超集\n",
    "        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)                    # 评估筛选满足最小可信度的规则列表\n",
    "        if (len(Hmp1) > 1):                                                          # 如果超集还可以合并，则继续递归\n",
    "            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, suppData = apriori(dataSet, minSupport=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({5}) --> frozenset({2}) conf: 1.0\n",
      "frozenset({2}) --> frozenset({5}) conf: 1.0\n",
      "frozenset({1}) --> frozenset({3}) conf: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(frozenset({5}), frozenset({2}), 1.0),\n",
       " (frozenset({2}), frozenset({5}), 1.0),\n",
       " (frozenset({1}), frozenset({3}), 1.0)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = generateRules(L, suppData, minConf=0.7)\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例：发现毒蘑菇的相似特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '3',\n",
       " '10',\n",
       " '14',\n",
       " '23',\n",
       " '26',\n",
       " '34',\n",
       " '36',\n",
       " '39',\n",
       " '41',\n",
       " '52',\n",
       " '55',\n",
       " '59',\n",
       " '63',\n",
       " '67',\n",
       " '76',\n",
       " '85',\n",
       " '86',\n",
       " '90',\n",
       " '93',\n",
       " '98',\n",
       " '108',\n",
       " '114']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mushDatSet = [line.split() for line in open(\"mushroom.dat\").readlines()]\n",
    "mushDatSet[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 23)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(mushDatSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, suppData = apriori(mushDatSet, minSupport=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'39', '2', '53'})\n",
      "frozenset({'2', '90', '53'})\n",
      "frozenset({'2', '85', '53'})\n",
      "frozenset({'2', '34', '53'})\n",
      "frozenset({'86', '2', '53'})\n",
      "frozenset({'28', '2', '53'})\n",
      "frozenset({'28', '39', '2'})\n",
      "frozenset({'28', '2', '90'})\n",
      "frozenset({'86', '28', '2'})\n",
      "frozenset({'28', '2', '85'})\n",
      "frozenset({'28', '63', '2'})\n",
      "frozenset({'59', '28', '2'})\n",
      "frozenset({'28', '2', '34'})\n",
      "frozenset({'39', '2', '93'})\n",
      "frozenset({'39', '2', '90'})\n",
      "frozenset({'39', '2', '85'})\n",
      "frozenset({'39', '2', '76'})\n",
      "frozenset({'39', '2', '67'})\n",
      "frozenset({'39', '63', '2'})\n",
      "frozenset({'39', '36', '2'})\n",
      "frozenset({'39', '2', '34'})\n",
      "frozenset({'2', '90', '93'})\n",
      "frozenset({'86', '2', '93'})\n",
      "frozenset({'86', '2', '90'})\n",
      "frozenset({'86', '2', '85'})\n",
      "frozenset({'86', '76', '2'})\n",
      "frozenset({'86', '2', '67'})\n",
      "frozenset({'86', '63', '2'})\n",
      "frozenset({'86', '36', '2'})\n",
      "frozenset({'86', '2', '34'})\n",
      "frozenset({'86', '23', '2'})\n",
      "frozenset({'86', '39', '2'})\n",
      "frozenset({'2', '85', '93'})\n",
      "frozenset({'90', '2', '85'})\n",
      "frozenset({'76', '2', '85'})\n",
      "frozenset({'76', '2', '34'})\n",
      "frozenset({'2', '85', '67'})\n",
      "frozenset({'63', '2', '93'})\n",
      "frozenset({'63', '2', '90'})\n",
      "frozenset({'63', '2', '85'})\n",
      "frozenset({'63', '2', '34'})\n",
      "frozenset({'59', '2', '93'})\n",
      "frozenset({'59', '2', '90'})\n",
      "frozenset({'59', '86', '2'})\n",
      "frozenset({'59', '2', '85'})\n",
      "frozenset({'59', '63', '2'})\n",
      "frozenset({'59', '36', '2'})\n",
      "frozenset({'59', '2', '34'})\n",
      "frozenset({'59', '23', '2'})\n",
      "frozenset({'59', '39', '2'})\n",
      "frozenset({'36', '2', '93'})\n",
      "frozenset({'36', '2', '90'})\n",
      "frozenset({'36', '2', '85'})\n",
      "frozenset({'63', '36', '2'})\n",
      "frozenset({'36', '2', '34'})\n",
      "frozenset({'2', '34', '93'})\n",
      "frozenset({'90', '2', '34'})\n",
      "frozenset({'85', '2', '34'})\n",
      "frozenset({'2', '34', '67'})\n",
      "frozenset({'23', '2', '93'})\n",
      "frozenset({'23', '2', '90'})\n",
      "frozenset({'23', '2', '85'})\n",
      "frozenset({'23', '63', '2'})\n",
      "frozenset({'23', '36', '2'})\n",
      "frozenset({'23', '2', '34'})\n",
      "frozenset({'23', '39', '2'})\n"
     ]
    }
   ],
   "source": [
    "# 在结果中搜索包含有毒特征值2的频繁项集\n",
    "for item in L[2]:\n",
    "    if item.intersection('2'):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'59', '28', '2', '39'})\n",
      "frozenset({'59', '28', '2', '34'})\n",
      "frozenset({'59', '28', '2', '63'})\n",
      "frozenset({'59', '28', '2', '85'})\n",
      "frozenset({'59', '28', '2', '86'})\n",
      "frozenset({'59', '28', '90', '2'})\n",
      "frozenset({'28', '2', '63', '34'})\n",
      "frozenset({'28', '2', '85', '63'})\n",
      "frozenset({'28', '2', '85', '34'})\n",
      "frozenset({'28', '2', '86', '53'})\n",
      "frozenset({'28', '2', '86', '39'})\n",
      "frozenset({'28', '2', '86', '34'})\n",
      "frozenset({'28', '2', '86', '63'})\n",
      "frozenset({'28', '2', '85', '86'})\n",
      "frozenset({'28', '90', '2', '86'})\n",
      "frozenset({'28', '2', '90', '34'})\n",
      "frozenset({'28', '2', '90', '85'})\n",
      "frozenset({'28', '2', '39', '34'})\n",
      "frozenset({'28', '2', '39', '63'})\n",
      "frozenset({'28', '2', '85', '39'})\n",
      "frozenset({'28', '2', '90', '39'})\n",
      "frozenset({'28', '2', '34', '53'})\n",
      "frozenset({'28', '2', '85', '53'})\n",
      "frozenset({'28', '2', '90', '53'})\n",
      "frozenset({'28', '2', '39', '53'})\n",
      "frozenset({'2', '86', '34', '53'})\n",
      "frozenset({'2', '85', '86', '53'})\n",
      "frozenset({'2', '90', '86', '53'})\n",
      "frozenset({'2', '86', '39', '53'})\n",
      "frozenset({'2', '85', '34', '53'})\n",
      "frozenset({'2', '90', '34', '53'})\n",
      "frozenset({'2', '90', '85', '53'})\n",
      "frozenset({'2', '39', '34', '53'})\n",
      "frozenset({'2', '85', '39', '53'})\n",
      "frozenset({'2', '90', '39', '53'})\n",
      "frozenset({'23', '2', '36', '34'})\n",
      "frozenset({'23', '2', '85', '36'})\n",
      "frozenset({'23', '2', '36', '93'})\n",
      "frozenset({'23', '2', '63', '34'})\n",
      "frozenset({'23', '36', '2', '63'})\n",
      "frozenset({'23', '2', '85', '63'})\n",
      "frozenset({'23', '2', '85', '34'})\n",
      "frozenset({'23', '2', '90', '34'})\n",
      "frozenset({'23', '2', '90', '85'})\n",
      "frozenset({'23', '2', '34', '93'})\n",
      "frozenset({'23', '2', '85', '93'})\n",
      "frozenset({'23', '2', '90', '93'})\n",
      "frozenset({'36', '2', '63', '34'})\n",
      "frozenset({'36', '2', '85', '63'})\n",
      "frozenset({'36', '2', '90', '63'})\n",
      "frozenset({'36', '2', '63', '93'})\n",
      "frozenset({'2', '85', '36', '34'})\n",
      "frozenset({'2', '90', '36', '34'})\n",
      "frozenset({'2', '90', '85', '36'})\n",
      "frozenset({'2', '36', '34', '93'})\n",
      "frozenset({'2', '85', '36', '93'})\n",
      "frozenset({'2', '90', '36', '93'})\n",
      "frozenset({'59', '23', '2', '34'})\n",
      "frozenset({'59', '23', '2', '36'})\n",
      "frozenset({'59', '23', '2', '63'})\n",
      "frozenset({'59', '23', '2', '85'})\n",
      "frozenset({'59', '23', '2', '86'})\n",
      "frozenset({'59', '23', '2', '90'})\n",
      "frozenset({'59', '23', '2', '93'})\n",
      "frozenset({'59', '2', '36', '34'})\n",
      "frozenset({'59', '2', '85', '36'})\n",
      "frozenset({'59', '2', '90', '36'})\n",
      "frozenset({'59', '2', '36', '93'})\n",
      "frozenset({'59', '2', '63', '34'})\n",
      "frozenset({'59', '36', '2', '63'})\n",
      "frozenset({'59', '2', '85', '63'})\n",
      "frozenset({'59', '2', '90', '63'})\n",
      "frozenset({'59', '2', '63', '93'})\n",
      "frozenset({'59', '2', '85', '34'})\n",
      "frozenset({'59', '2', '86', '34'})\n",
      "frozenset({'59', '2', '86', '36'})\n",
      "frozenset({'59', '2', '86', '63'})\n",
      "frozenset({'59', '2', '85', '86'})\n",
      "frozenset({'59', '2', '90', '86'})\n",
      "frozenset({'59', '2', '86', '93'})\n",
      "frozenset({'59', '2', '90', '34'})\n",
      "frozenset({'59', '2', '90', '85'})\n",
      "frozenset({'59', '2', '34', '93'})\n",
      "frozenset({'59', '2', '85', '93'})\n",
      "frozenset({'59', '2', '90', '93'})\n",
      "frozenset({'2', '85', '63', '34'})\n",
      "frozenset({'2', '90', '63', '34'})\n",
      "frozenset({'2', '90', '85', '63'})\n",
      "frozenset({'2', '63', '34', '93'})\n",
      "frozenset({'2', '85', '63', '93'})\n",
      "frozenset({'2', '90', '63', '93'})\n",
      "frozenset({'2', '85', '67', '34'})\n",
      "frozenset({'76', '2', '85', '34'})\n",
      "frozenset({'90', '2', '85', '34'})\n",
      "frozenset({'2', '85', '34', '93'})\n",
      "frozenset({'23', '2', '86', '34'})\n",
      "frozenset({'23', '2', '86', '36'})\n",
      "frozenset({'23', '2', '85', '86'})\n",
      "frozenset({'23', '2', '90', '86'})\n",
      "frozenset({'23', '2', '86', '93'})\n",
      "frozenset({'2', '86', '36', '34'})\n",
      "frozenset({'2', '85', '86', '36'})\n",
      "frozenset({'2', '90', '86', '36'})\n",
      "frozenset({'2', '86', '36', '93'})\n",
      "frozenset({'23', '2', '86', '63'})\n",
      "frozenset({'2', '86', '63', '34'})\n",
      "frozenset({'36', '2', '86', '63'})\n",
      "frozenset({'2', '85', '86', '63'})\n",
      "frozenset({'2', '90', '86', '63'})\n",
      "frozenset({'2', '86', '63', '93'})\n",
      "frozenset({'2', '67', '86', '34'})\n",
      "frozenset({'76', '2', '86', '34'})\n",
      "frozenset({'76', '2', '85', '86'})\n",
      "frozenset({'2', '85', '86', '34'})\n",
      "frozenset({'2', '85', '67', '86'})\n",
      "frozenset({'2', '90', '86', '34'})\n",
      "frozenset({'2', '90', '85', '86'})\n",
      "frozenset({'2', '86', '34', '93'})\n",
      "frozenset({'2', '85', '86', '93'})\n",
      "frozenset({'2', '90', '86', '93'})\n",
      "frozenset({'2', '90', '34', '93'})\n",
      "frozenset({'2', '90', '85', '93'})\n",
      "frozenset({'23', '2', '39', '34'})\n",
      "frozenset({'23', '2', '39', '36'})\n",
      "frozenset({'23', '2', '39', '63'})\n",
      "frozenset({'23', '2', '85', '39'})\n",
      "frozenset({'23', '2', '39', '93'})\n",
      "frozenset({'2', '39', '36', '34'})\n",
      "frozenset({'2', '85', '39', '36'})\n",
      "frozenset({'2', '90', '39', '36'})\n",
      "frozenset({'2', '39', '36', '93'})\n",
      "frozenset({'59', '23', '2', '39'})\n",
      "frozenset({'59', '2', '39', '34'})\n",
      "frozenset({'59', '2', '39', '36'})\n",
      "frozenset({'59', '2', '39', '63'})\n",
      "frozenset({'59', '2', '85', '39'})\n",
      "frozenset({'59', '2', '86', '39'})\n",
      "frozenset({'59', '90', '2', '39'})\n",
      "frozenset({'59', '2', '39', '93'})\n",
      "frozenset({'2', '39', '63', '34'})\n",
      "frozenset({'36', '2', '39', '63'})\n",
      "frozenset({'2', '85', '39', '63'})\n",
      "frozenset({'2', '90', '39', '63'})\n",
      "frozenset({'2', '39', '63', '93'})\n",
      "frozenset({'2', '67', '39', '34'})\n",
      "frozenset({'2', '76', '39', '34'})\n",
      "frozenset({'2', '85', '39', '34'})\n",
      "frozenset({'2', '85', '67', '39'})\n",
      "frozenset({'2', '85', '76', '39'})\n",
      "frozenset({'23', '2', '86', '39'})\n",
      "frozenset({'2', '86', '39', '34'})\n",
      "frozenset({'2', '86', '39', '36'})\n",
      "frozenset({'2', '86', '39', '63'})\n",
      "frozenset({'2', '67', '86', '39'})\n",
      "frozenset({'76', '2', '86', '39'})\n",
      "frozenset({'2', '85', '86', '39'})\n",
      "frozenset({'90', '2', '86', '39'})\n",
      "frozenset({'2', '86', '39', '93'})\n",
      "frozenset({'2', '90', '39', '34'})\n",
      "frozenset({'2', '90', '85', '39'})\n",
      "frozenset({'2', '39', '34', '93'})\n",
      "frozenset({'2', '85', '39', '93'})\n",
      "frozenset({'2', '90', '39', '93'})\n"
     ]
    }
   ],
   "source": [
    "for item in L[3]:\n",
    "    if item.intersection('2'):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
